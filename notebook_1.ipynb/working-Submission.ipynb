{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # CPU version\n",
    "from sklearn.linear_model import LogisticRegression      # CPU version\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "#############################################\n",
    "# 1. Data Loading and Preprocessing\n",
    "#############################################\n",
    "def load_data():\n",
    "    train_features = pd.read_csv('train-features.csv')\n",
    "    train_labels = pd.read_csv('train-labels.csv')\n",
    "    df = pd.merge(train_features, train_labels, on='walmart_id')\n",
    "    print(\"Training data shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "def preprocess_text(df):\n",
    "    \"\"\"\n",
    "    Combines title, details_Manufacturer, and store into one text field.\n",
    "    To boost manufacturer information (which is crucial for brand prediction),\n",
    "    we duplicate details_Manufacturer twice.\n",
    "    \"\"\"\n",
    "    df['text_features'] = (\n",
    "        df['title'].fillna('') + ' ' +\n",
    "        (df['details_Manufacturer'].fillna('') + ' ') * 2 +\n",
    "        df['store'].fillna('')\n",
    "    ).str.lower()\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "df = preprocess_text(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2. TF‑IDF Vectorization\n",
    "#############################################\n",
    "# Use a moderate feature space to balance nuance with memory\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), \n",
    "                             stop_words='english', sublinear_tf=True)\n",
    "X_sparse = vectorizer.fit_transform(df['text_features'])\n",
    "# Convert sparse matrix to dense array and use float32 to save memory\n",
    "X = X_sparse.toarray().astype(np.float32)\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 3. Label Encoding for All Targets\n",
    "#############################################\n",
    "# We target six outputs:\n",
    "# - Main outputs: L0_category, L1_category, L2_category, details_Brand\n",
    "# - Separate outputs: L3_category, L4_category\n",
    "all_labels = ['L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category', 'details_Brand']\n",
    "encoders = {}\n",
    "for col in all_labels:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Create label arrays:\n",
    "main_cols = ['L0_category', 'L1_category', 'L2_category', 'details_Brand']\n",
    "y_main = np.column_stack([df[col] for col in main_cols])\n",
    "y_L3 = df['L3_category'].values\n",
    "y_L4 = df['L4_category'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 4. Train/Test Split\n",
    "#############################################\n",
    "# We use the same split for all models. Save indices for consistency.\n",
    "indices = np.arange(X.shape[0])\n",
    "X_train, X_val, idx_train, idx_val = train_test_split(X, indices, test_size=0.2, random_state=42)\n",
    "y_main_train = np.column_stack([df[col].values[idx_train] for col in main_cols])\n",
    "y_main_val = np.column_stack([df[col].values[idx_val] for col in main_cols])\n",
    "y_L3_train = y_L3[idx_train]\n",
    "y_L3_val = y_L3[idx_val]\n",
    "y_L4_train = y_L4[idx_train]\n",
    "y_L4_val = y_L4[idx_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 5. Train Main Multi‑Output Model for Main Outputs\n",
    "#############################################\n",
    "base_classifier = LogisticRegression(max_iter=1000, C=20.0)\n",
    "model_main = MultiOutputClassifier(base_classifier)\n",
    "model_main.fit(X_train, y_main_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 6. Train Separate Models for L3 and L4\n",
    "#############################################\n",
    "# We use a higher C value (e.g. 50.0) for L3 and L4 to give them extra flexibility.\n",
    "model_L3 = LogisticRegression(max_iter=1000, C=50.0)\n",
    "model_L3.fit(X_train, y_L3_train)\n",
    "\n",
    "model_L4 = LogisticRegression(max_iter=1000, C=50.0)\n",
    "model_L4.fit(X_train, y_L4_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 7. Evaluate the Models on the Validation Set\n",
    "#############################################\n",
    "print(\"\\n--- Main Outputs Evaluation ---\")\n",
    "y_main_pred = model_main.predict(X_val)\n",
    "for i, col in enumerate(main_cols):\n",
    "    print(f\"\\nClassification Report for {col}:\")\n",
    "    print(classification_report(y_main_val[:, i], y_main_pred[:, i]))\n",
    "\n",
    "print(\"\\n--- L3 Evaluation ---\")\n",
    "y_L3_pred = model_L3.predict(X_val)\n",
    "print(classification_report(y_L3_val, y_L3_pred))\n",
    "\n",
    "print(\"\\n--- L4 Evaluation ---\")\n",
    "y_L4_pred = model_L4.predict(X_val)\n",
    "print(classification_report(y_L4_val, y_L4_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 8. Save the Models and Vectorizer\n",
    "#############################################\n",
    "joblib.dump(model_main, 'product_classifier_main.joblib')\n",
    "joblib.dump(model_L3, 'product_classifier_L3.joblib')\n",
    "joblib.dump(model_L4, 'product_classifier_L4.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(encoders, 'label_encoders.joblib')\n",
    "print(\"\\nModels, vectorizer, and encoders saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 9. Inference Example Function\n",
    "#############################################\n",
    "def predict_categories(text, vectorizer, model_main, model_L3, model_L4, encoders):\n",
    "    X_new = vectorizer.transform([text]).toarray().astype(np.float32)\n",
    "    preds_main = model_main.predict(X_new)\n",
    "    pred_L3 = model_L3.predict(X_new)\n",
    "    pred_L4 = model_L4.predict(X_new)\n",
    "    results = {}\n",
    "    for i, col in enumerate(main_cols):\n",
    "        results[col] = encoders[col].inverse_transform([preds_main[0][i]])[0]\n",
    "    results['L3_category'] = encoders['L3_category'].inverse_transform([pred_L3[0]])[0]\n",
    "    results['L4_category'] = encoders['L4_category'].inverse_transform([pred_L4[0]])[0]\n",
    "    return results\n",
    "\n",
    "test_text = \"Nike Air Max Running Shoes for Men\"\n",
    "predictions = predict_categories(test_text, vectorizer, model_main, model_L3, model_L4, encoders)\n",
    "print(\"\\nPredicted categories for:\", test_text)\n",
    "for cat, pred in predictions.items():\n",
    "    print(f\"{cat}: {pred}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 10. Batch Prediction and Submission File Creation\n",
    "#############################################\n",
    "test_df = pd.read_csv(\"test-features.csv\")\n",
    "test_df[\"text_features\"] = (\n",
    "    test_df[\"title\"].fillna(\"\") + ' ' +\n",
    "    (test_df[\"details_Manufacturer\"].fillna(\"\") + ' ') * 2 +\n",
    "    test_df[\"store\"].fillna(\"\")\n",
    ").str.lower()\n",
    "\n",
    "X_test = vectorizer.transform(test_df[\"text_features\"]).toarray().astype(np.float32)\n",
    "preds_main = model_main.predict(X_test)\n",
    "preds_L3 = model_L3.predict(X_test)\n",
    "preds_L4 = model_L4.predict(X_test)\n",
    "\n",
    "predictions_list = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    sample_pred = {}\n",
    "    for j, col in enumerate(main_cols):\n",
    "        sample_pred[col] = encoders[col].inverse_transform([preds_main[i][j]])[0]\n",
    "    sample_pred['L3_category'] = encoders['L3_category'].inverse_transform([preds_L3[i]])[0]\n",
    "    sample_pred['L4_category'] = encoders['L4_category'].inverse_transform([preds_L4[i]])[0]\n",
    "    predictions_list.append(sample_pred)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "submission_df = pd.concat([test_df[[\"walmart_id\"]], predictions_df], axis=1)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved as submission.csv.\")\n",
    "\n",
    "# Optionally, if using Google Colab, download the file:\n",
    "from google.colab import files\n",
    "files.download(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
